# data-no-bpe.yaml

    ## Where the samples will be written
    save_data: models/2025-03-25-09-44/example
    ## Where the vocab(s) will be written
    src_vocab: vocab/example.vocab.src
    tgt_vocab: vocab/example.vocab.tgt
    # Prevent overwriting existing files in the folder
    overwrite: True

    # Corpus opts:
    data:
        corpus_1:
            path_src: data/src-train-token.txt
            path_tgt: data/tgt-train-token.txt
        valid:
            path_src: data/src-val-token.txt
            path_tgt: data/tgt-val-token.txt


    # Where to save the checkpoints
    save_model: models/2025-03-25-09-44/model
    save_checkpoint_steps: 10000
    valid_steps: 10000
    train_steps: 200000

    # Batching
    bucket_size: 262144
    world_size: 4
    
    num_workers: 2
    batch_type: "tokens"
    batch_size: 4096
    valid_batch_size: 2048
    accum_count: [4]
    accum_steps: [0]

    # Optimization
    model_dtype: "fp16"
    optim: "adam"
    learning_rate: 2
    warmup_steps: 8000
    decay_method: "noam"
    adam_beta2: 0.998
    max_grad_norm: 0
    label_smoothing: 0.1
    param_init: 0
    param_init_glorot: true
    normalization: "tokens"

    # Model
    encoder_type: transformer
    decoder_type: transformer
    position_encoding: true
    enc_layers: 6
    dec_layers: 6
    heads: 8
    rnn_size: 512
    word_vec_size: 512
    transformer_ff: 2048
    dropout_steps: [0]
    dropout: [0.1]
    attention_dropout: [0.1]


    world_size: 1
    gpu_ranks:
    - 0         
    